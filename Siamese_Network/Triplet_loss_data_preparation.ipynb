{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ROOUzntitiz8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = \"cropped_train_dataset\"\n",
        "test_dataset = \"cropped_test_dataset\"\n",
        "\n",
        "image_size = (224, 224)\n",
        "\n",
        "batch_size = 256\n",
        "buffer_size = batch_size * 2\n",
        "\n",
        "auto = tf.data.AUTOTUNE\n",
        "\n",
        "LEARNING_RATE = 0.0001\n",
        "STEPS_PER_EPOCH = 50\n",
        "VALIDATION_STEPS = 10\n",
        "EPOCHS = 10\n",
        "\n",
        "output_path = \"output\"\n",
        "model_path = os.path.join(output_path, \"siamese_network\")\n",
        "output_image_path = os.path.join(output_path, \"output_image.png\")"
      ],
      "metadata": {
        "id": "fjvScjP1tlhC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "r8MNU7mxyQud"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MapFunction():\n",
        "  def __init__(self, imageSize):\n",
        "    self.imageSize = imageSize\n",
        "\n",
        "  def decode_and_resize(self, imagePath):\n",
        "    image = tf.io.read_file(imagePath)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    image = tf.image.resize(image, self.imageSize)\n",
        "\n",
        "    return image\n",
        "\n",
        "  def __call__(self, anchor, positive, negative):\n",
        "    anchor = self.decode_and_resize(anchor)\n",
        "    positive = self.decode_and_resize(positive)\n",
        "    negative = self.decode_and_resize(negative)\n",
        "    return (anchor, positive, negative)"
      ],
      "metadata": {
        "id": "9ELegywEyZev"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TripletGenerator:\n",
        "  def __init__(self, datasetPath):\n",
        "    self.peopleNames=list()\n",
        "\n",
        "    for folderName in os.listdir(datasetPath):\n",
        "      absoluteFolderName = os.path.join(datasetPath, folderName)\n",
        "\n",
        "      numImages = len(os.listdir(absoluteFolderName))\n",
        "\n",
        "      if numImages > 1:\n",
        "        self.peopleNames.append(absoluteFolderName)\n",
        "\n",
        "      self.allPeople = generate_all_people_dict()\n",
        "\n",
        "  def generate_all_people_dict(self):\n",
        "    allPeople = dict()\n",
        "\n",
        "    for personName in self.peopleNames:\n",
        "      imageNames = os.listdir(personName)\n",
        "\n",
        "      personPhotos = [\n",
        "          os.path.join(personName, imageName) for imageName in imageNames\n",
        "      ]\n",
        "      allPeople[personName] = personPhotos\n",
        "\n",
        "    return allPeople\n",
        "\n",
        "  def get_next_element(self):\n",
        "    while True:\n",
        "      anchorName = random.choice(self.peopleNames)\n",
        "      temporaryNames =self.peopleNames.copy()\n",
        "      temporaryNames.remove(anchorName)\n",
        "      negativeName = random.choice(temporaryNames)\n",
        "      # draw two images from the anchor folder without replacement\n",
        "      (anchorPhoto, positivePhoto) = np.random.choice(\n",
        "\t\t\t\ta=self.allPeople[anchorName],\n",
        "\t\t\t\tsize=2,\n",
        "\t\t\t\treplace=False\n",
        "\t\t\t)\n",
        "\t\t\t# draw an image from the negative folder\n",
        "      negativePhoto = random.choice(self.allPeople[negativeName])\n",
        "\t\t\t# yield the anchor, positive and negative photos\n",
        "      yield (anchorPhoto, positivePhoto, negativePhoto)"
      ],
      "metadata": {
        "id": "EA7uZKqtzq7-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imutils.paths import list_images\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os"
      ],
      "metadata": {
        "id": "TvX0TBGvUUtv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/siamese-201/siamese-201.zip\n",
        "!unzip -qq siamese-201.zip\n",
        "%cd siamese-201"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4o4CMeo0LTu",
        "outputId": "ad2d21d4-9104-426a-a0df-29095cf658e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-14 02:26:09--  https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/siamese-201/siamese-201.zip\n",
            "Resolving pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com (pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com)... 52.92.177.2, 52.92.144.162, 52.218.177.89, ...\n",
            "Connecting to pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com (pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com)|52.92.177.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11107236 (11M) [binary/octet-stream]\n",
            "Saving to: ‘siamese-201.zip’\n",
            "\n",
            "siamese-201.zip     100%[===================>]  10.59M  20.4MB/s    in 0.5s    \n",
            "\n",
            "2023-08-14 02:26:09 (20.4 MB/s) - ‘siamese-201.zip’ saved [11107236/11107236]\n",
            "\n",
            "/content/siamese-201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O lfw.tgz\n",
        "! tar -zxf lfw.tgz\n",
        "! mv \"lfw-deepfunneled\" \"train_dataset\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8jpbAa43XGA",
        "outputId": "a78dd62a-304e-4b17-c763-26cbea310a99"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-14 02:26:47--  http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n",
            "Resolving vis-www.cs.umass.edu (vis-www.cs.umass.edu)... 128.119.244.95\n",
            "Connecting to vis-www.cs.umass.edu (vis-www.cs.umass.edu)|128.119.244.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 108761145 (104M) [application/x-gzip]\n",
            "Saving to: ‘lfw.tgz’\n",
            "\n",
            "lfw.tgz             100%[===================>] 103.72M  1.55MB/s    in 88s     \n",
            "\n",
            "2023-08-14 02:28:16 (1.17 MB/s) - ‘lfw.tgz’ saved [108761145/108761145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = {\n",
        "\t\"dataset\": \"train_dataset\",\n",
        "\t\"output\": \"cropped_train_dataset\",\n",
        "\t\"prototxt\": \"face_crop_model/deploy.prototxt.txt\",\n",
        "\t\"model\": \"face_crop_model/res10_300x300_ssd_iter_140000.caffemodel\",\n",
        "\t\"confidence\": 0.5,\n",
        "}\n",
        "\n",
        "test_args = {\n",
        "\t\"dataset\": \"test_dataset\",\n",
        "\t\"output\": \"cropped_test_dataset\",\n",
        "\t\"prototxt\": \"face_crop_model/deploy.prototxt.txt\",\n",
        "\t\"model\": \"face_crop_model/res10_300x300_ssd_iter_140000.caffemodel\",\n",
        "\t\"confidence\": 0.5,\n",
        "}"
      ],
      "metadata": {
        "id": "JmqBiuBN3gh2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = train_args"
      ],
      "metadata": {
        "id": "udFH1Q7b5CIY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load our serialized model from disk\n",
        "print(\"[INFO] loading model...\")\n",
        "net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
        "\n",
        "# check if the output dataset directory exists, if it doesn't, then\n",
        "# create it\n",
        "if not os.path.exists(args[\"output\"]):\n",
        "\tos.makedirs(args[\"output\"])\n",
        "\n",
        "# grab the file and sub-directory names in dataset directory\n",
        "print(\"[INFO] grabbing the names of files and directories...\")\n",
        "names = os.listdir(args[\"dataset\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7DTceCq4l7K",
        "outputId": "923f2e48-342c-470d-8cb5-c489fc7d99a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading model...\n",
            "[INFO] grabbing the names of files and directories...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over all names\n",
        "print(\"[INFO] starting to crop faces and saving them to disk...\")\n",
        "for name in tqdm(names):\n",
        "\t# build directory path\n",
        "\tdirPath = os.path.join(args[\"dataset\"], name)\n",
        "\n",
        "\t# check if the directory path is a directory\n",
        "\tif os.path.isdir(dirPath):\n",
        "\t\t# grab the path to all the images in the directory\n",
        "\t\timagePaths = list(list_images(dirPath))\n",
        "\n",
        "\t\t# build the path to the output directory\n",
        "\t\toutputDir = os.path.join(args[\"output\"], name)\n",
        "\n",
        "\t\t# check if the output directory exists, if it doesn't, then\n",
        "\t\t# create it\n",
        "\t\tif not os.path.exists(outputDir):\n",
        "\t\t\tos.makedirs(outputDir)\n",
        "\n",
        "\t\t# loop over all image paths\n",
        "\t\tfor imagePath in imagePaths:\n",
        "\t\t\t# grab the image ID, load the image, and grab the\n",
        "\t\t\t# dimensions of the image\n",
        "\t\t\timageID = imagePath.split(os.path.sep)[-1]\n",
        "\t\t\timage = cv2.imread(imagePath)\n",
        "\t\t\t(h, w) = image.shape[:2]\n",
        "\n",
        "\t\t\t# construct an input blob for the image by resizing to a\n",
        "\t\t\t# fixed 300x300 pixels and then normalizing it\n",
        "\t\t\tblob = cv2.dnn.blobFromImage(cv2.resize(image,\n",
        "\t\t\t\t(300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "\n",
        "\t\t\t# pass the blob through the network and obtain the\n",
        "\t\t\t# detections and predictions\n",
        "\t\t\tnet.setInput(blob)\n",
        "\t\t\tdetections = net.forward()\n",
        "\n",
        "\t\t\t# extract the index of the detection with max\n",
        "\t\t\t# probability and get the maximum confidence value\n",
        "\t\t\ti = np.argmax(detections[0, 0, :, 2])\n",
        "\t\t\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t\t\t# filter out weak detections by ensuring the\n",
        "\t\t\t# `confidence` is greater than the minimum confidence\n",
        "\t\t\tif confidence > args[\"confidence\"]:\n",
        "\t\t\t\t# grab the maximum dimension value\n",
        "\t\t\t\tmaxDim = np.max(detections[0, 0, i, 3:7])\n",
        "\n",
        "\t\t\t\t# check if max dimension value is greater than one,\n",
        "\t\t\t\t# if so, skip the detection since it is erroneous\n",
        "\t\t\t\tif maxDim > 1.0:\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t# clip the dimension values to be between 0 and 1\n",
        "\t\t\t\tbox = np.clip(detections[0, 0, i, 3:7], 0.0, 1.0)\n",
        "\n",
        "\t\t\t\t# compute the (x, y)-coordinates of the bounding\n",
        "\t\t\t\t# box for the object\n",
        "\t\t\t\tbox = box * np.array([w, h, w, h])\n",
        "\t\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t\t# grab the face from the image, build the path to\n",
        "\t\t\t\t# the output face image, and write it to disk\n",
        "\t\t\t\tface = image[startY:endY,startX:endX,:]\n",
        "\t\t\t\tfacePath = os.path.join(outputDir, imageID)\n",
        "\t\t\t\tcv2.imwrite(facePath, face)\n",
        "\n",
        "print(\"[INFO] finished cropping faces and saving them to disk...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEvLnu7C4yax",
        "outputId": "3d3d740b-0da3-4f50-eaf9-9ae2391ea2b3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] starting to crop faces and saving them to disk...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5749/5749 [11:41<00:00,  8.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] finished cropping faces and saving them to disk...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "glig-jni5Hig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}